\chapter*{\Large \center Abstract}

% Guidance of how to write an abstract/summary provided by Nature: https://cbs.umn.edu/sites/cbs.umn.edu/files/public/downloads/Annotated_Nature_abstract.pdf

Compact Muon Solenoid (CMS) detector was built in the middle of collision from LargeHadron Collider (LHC) which is one of the most powerful particle accelerators in the world. The mission is to collect the product from collision and decaying which happens 40 million times each second. The data taking in the CMS experiment is reconstructed to become a physics quantity 48 hours after a collision. The certification of data quality is made on run and lumisection levels. The criteria to certify are both from an automatic system as well as manual work from untraceable misbehaving of detector which is marked by offline shifter and detector experts. Approximately 95\% of data are good and the rest of them are bad. It is not easy to say that all phenomena that cause misbehaving of a result are well understood. Then the aim of this work is to reduce the manual work for data qualification by exploding various types of semi-supervised learning by treating the outlier as bad in lumisection granularity.