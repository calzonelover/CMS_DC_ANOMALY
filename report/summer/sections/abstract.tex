\chapter*{\Large \center Abstract}

% Guidance of how to write an abstract/summary provided by Nature: https://cbs.umn.edu/sites/cbs.umn.edu/files/public/downloads/Annotated_Nature_abstract.pdf

Compact Muon Solenoid (CMS) detector was built in the middle of collision from Large Hadron Collider (LHC) which is one of the most powerful particle accelerator in the world. The mission is to collect the product from collision and decaying which happens 40 million times each second.
The data taking in CMS experiment is reconstructed to become physics quantity 48 hours after collision. The certification of data quality is made on run and lumisection levels. The criteria to certify are both from an automatic system as well as mannual work from untracable misbehaving of detector which are marked by offline shifer and detector experts.
Approximately 95\% of data are good and the rest of them are bad. It is not easy to say that all phenomenon that cause misbehaving of a result are well understood.
Then the aim of this work is to reduce the mannual work for data qualification by exploiding various types of semi-supervised learning by treating the outlier as bad in lumisection granularity.