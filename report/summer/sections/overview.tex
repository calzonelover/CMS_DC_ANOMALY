\chapter{Overview}

Before the whole data be feeding to physics analysis, there are a procedure to certify the data quality in run and lumisection granularity.
Data quality monitoring (DQM) team provide the tools and workflow where there are offline and online section to consider which basically online is a real time and the data that we get 2 days after collision would be inspected in offline section.
There are the person who looking at a dozen of histogram that demenstrate the occupancy of detector in each sub-system and also the physics quantities in various perspective of information.
Most of Bad lumisection (LS) are automatically came from run tagged as bad by human for the whole run and DCS bits in lumisection levels. In some cases, there are a small fraction bad LS that are manually marked as bad by data certification experts in lumisection levels because some kind of detector malfunction isn't tracable from the previous process.
On the other hand, good LS are defined in Golden JSON which are literally taken from data that passes of of those bad criteria.

The main objective of this work is to find a mathematical way to certify data quality in lumisection granularity to reduce the mannual work of data certification experts.
As you might have seen that there are only a small fraction of data. Moreover, the bad data that are marked by the expert are relatively small compare to a good data.
Then we have to face to the imbalance class problem and it is the reason why we choosing semi-supervised learing where we feed only good data to train the unsupervise model and testing with both kind of data.
Consequenly, the data that are mark as bad by the model would be consider as outlier where model does not familier with. The more explicit analysis would be provided in this report.