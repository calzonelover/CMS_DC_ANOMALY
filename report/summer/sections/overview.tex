\chapter{Overview}

Before the whole data be feeding to physics analysis, there is a procedure to certify the data quality in run and lumisection granularity. Data quality monitoring (DQM) team provides the tools and workflow where there is an offline and online section to consider which basically online is a real-time and the data that we get 2 days after collision would be inspected in offline section. The person who looking at a dozen of a histogram that demonstrates the occupancy of the detector in each sub-system and also the physics quantities in various perspectives of information. Most of Bad lumisection (LS) are automatically came from run tagged as bad by a human for the whole run and DCS bits in lumisection levels. In some cases, there is a small fraction bad LS that are manually marked as bad by data certification experts in lumisection levels because some kind of detector malfunction isnâ€™t traceable from the previous process. On the other hand, good LS is defined in Golden JSON which is literally taken from data that passes of those bad criteria.

The main objective of this work is to find a mathematical way to certify data quality lumisection granularity to reduce the manual work of data certification experts. As it can be seen that there are only a small fraction of data. Moreover, the bad data that are marked by the expert are relatively small compare to good data. Then we have to face to the imbalanced class problem and it is the reason why we choosing semi-supervised learning where we feed only good data to train the unsupervised model and testing with both kinds of data. Consequently, the data that are marked as bad by the model would be considered an outlier where the model does not familiar with. A more explicit analysis would be provided in this report.